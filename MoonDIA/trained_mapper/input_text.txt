Abstract
Autonomous visual navigation is an essential element in robot autonomy. Reinforcement learning (RL) offers a promising policy training paradigm. However existing RL methods suffer from high sample complexity, poor sim-to-real transfer, and limited runtime adaptability to navigation scenarios not seen during training. These problems are particularly challenging for drones, with complex nonlinear and unstable dynamics, and strong dynamic coupling between control and perception. In this paper, we propose a novel framework that integrates 3D Gaussian Splatting (3DGS) with differentiable deep reinforcement learning (DDRL) to train vision-based drone navigation policies. By leveraging high-fidelity 3D scene representations and differentiable simulation, our method improves sample efficiency and sim-to-real transfer. Additionally, we incorporate a Context-aided Estimator Network (CENet) to adapt to environmental variations at runtime. Moreover, by curriculum training in a mixture of different surrounding environments, we achieve in-task generalization, the ability to solve new instances of a task not seen during training. Drone hardware experiments demonstrate our methodâ€™s high training efficiency compared to state-of-the-art RL methods, zero shot sim-to-real transfer for real robot deployment without fine tuning, and ability to adapt to new instances within the same task class (e.g. to fly through a gate at different locations with different distractors in the environment).

IIntroduction
Autonomous drones have the potential to improve practices in agriculture, environmental management, and search and rescue [1, 2, 3]. A critical aspect of these robotsâ€™ functionality is their efficient navigation and control in complex, unstructured environments [4]. Traditional approaches to this problem have predominantly relied on a stack of different modules including perception, localization, mapping, planning, and control [5], [6]. However, the integration of these different modules has many issues, including high system complexity and computational overhead, communication latency between modules, multiple points of failure, and difficult-to-characterize error propagation between modules.

Recent advancements in machine learning have offered alternative solutions to these challenges. Deep reinforcement learning (DRL) has been proven to be successful in drone racing [7]. By leveraging DRL, robots can potentially learn to directly map sensor inputs to control outputs, bypassing the need for explicit modular separation [8]. While promising, so far, these DRL solutions have been mostly limited to drone racing-like tasks but have not proved successful in navigating complex and unstructured environments. To tackle the challenge, one of the most important bottlenecks lies on the difficulty in getting high-quality perception data when training the policy in conventional simulators [9, 10]. Synthetic visual data still struggles to accurately capture fine details, often leading to a substantial sim-to-real gap [11].

3D reconstruction has demonstrated exceptional capability in generating high-fidelity 3D scene representations from sparse views [12, 13], making it an attractive approach for creating an environment model [14, 15, 16]. Flying in 3D reconstruction has been proven successful for multiple drone tasks including trajectory planning, pose estimation, and visual navigation [17, 18, 19, 20, 21, 22]. Moreover, the recent work SOUS VIDE [23] utilizes Flying in a Gaussian Splat (FiGS) to train end-to-end drone navigation policies, and demonstrated successful sim-to-real transfer. However, the works above are based on imitation learning, which requires a large amount of high-quality expert pilot data, long training time, and suffers from a lack of generalization to new task instances.

On the other hand, differentiable simulators [24, 25, 26, 27] and differentiable deep reinforcement learning (DDRL) algorithms [28], [29] alleviate another critical issue of sample efficiency for conventional DRL. Differentiable simulators not only provide the simulated objectâ€™s next state, but also delivers the gradient of the simulated state with respect to the action and previous state, enhancing model training with gradient knowledge rather than simply trial and error. By leveraging a custom smoothing critic function and truncated learning window, the Short Horizon Actor-Critic (SHAC) algorithm [29] alleviates the problems of local minima and exploding/vanishing gradients that commonly occur in DDRL training.

Even with exteroceptive sensors like RGB cameras, it is still a significant challenge for drone policies to understand the complex surrounding environment, especially the spatial relationship between drone and obstacle. To address this, we introduce a context-aided estimator network (CENet) [30] positioned before the policy network, enabling the agent to infer crucial environmental properties at runtime. While previous approaches have primarily relied on proprioceptive sensing, we extend this idea by including RGB camera sensor inputs, leading to improved in-task adaptability in navigation scenarios. Concretely, CENet outputs a latent embedding at runtime summarizing visual and dynamical context. The policy is conditioned on this code, enabling the drone to modify its behavior in response to the sensed runtime environment.

To achieve the goal of visual-motor navigation, we propose a novel approach that leverages 3DGS in conjunction with DDRL, using SHAC-like training algorithm and a policy architecture augmented with a CENet module for in-context runtime adaptation. By utilizing this combination, we develop a visual navigation policy learning framework that transfers to real-world drone navigation performance. Our main contributions are:

â€¢ We introduce a simulator for training robot visuomotor policies by integrating 3DGS for high-fidelity visuals with a differentiable dynamics model for computing end-to-end gradients.
â€¢ We propose GRaD-Nav, a DDRL algorithm designed for efficient end-to-end visual navigation policy learning, achieving higher sample efficiency than previous approaches.
â€¢ We demonstrate in-task generalization by training a single policy for the drone to fly through gates at different locations with different distractors in the environment not seen at training time.
IIBackground
II-ADifferentiable Simulation and Differentiable Reinforcement Learning
II-A1Differentiable Dynamics Simulation
Conceptually, we view the simulator as a differentiable abstract function, 
ğ¬
t
+
1
=
â„±
â¢
(
ğ¬
t
,
ğš
t
)
, which transitions a state 
ğ¬
 from time 
t
 to 
t
+
1
. Here, 
ğš
t
 represents the control input during that time-step. Given a differentiable scalar loss function 
â„’
 and its adjoint 
â„’
âˆ—
=
âˆ‚
â„’
âˆ‚
ğ¬
t
+
1
, the backward pass of the simulator computes:

âˆ‚
â„’
âˆ‚
ğ¬
t
=
(
âˆ‚
â„’
âˆ‚
ğ¬
t
+
1
)
â¢
(
âˆ‚
â„±
âˆ‚
ğ¬
t
)
,
âˆ‚
â„’
âˆ‚
ğš
t
=
(
âˆ‚
â„’
âˆ‚
ğ¬
t
+
1
)
â¢
(
âˆ‚
â„±
âˆ‚
ğš
t
)
.
(1)
II-A2Short-Horizon Actor-Critic
The Short-Horizon Actor-Critic method (SHAC) [29] was introduced to address the challenges associated with gradient-based policy learning. This method simultaneously trains a policy network (actor) 
Ï€
Î¸
 and a value network (critic) 
V
Ï•
 during task execution, dividing the entire task horizon into several smaller sub-windows across learning episodes. Differentiable simulation allows for backpropagation of the gradient through states and actions within the sub-windows, providing an accurate policy gradient.

The SHAC method operates in an on-policy manner as follows: In each learning episode, the algorithm samples 
N
 trajectories 
{
Ï„
i
}
 of short horizon 
h
â‰ª
H
 in parallel from the simulation, continuing from the end states of the trajectories in the previous learning episode. The policy loss is then computed as follows:

â„’
Î¸
=
âˆ’
1
N
â¢
h
â¢
âˆ‘
i
=
1
N
[
(
âˆ‘
t
=
t
0
t
0
+
h
âˆ’
1
Î³
t
âˆ’
t
0
â¢
â„›
â¢
(
ğ¬
t
i
,
ğš
t
i
)
)
+
Î³
h
â¢
V
Ï•
â¢
(
ğ¬
t
0
+
h
i
)
]
,
(2)
In this context, 
ğ¬
t
i
 and 
ğš
t
i
 represent the state and action at step 
t
 of the 
i
-th trajectory, and 
Î³
<
1
 is a discount factor used to stabilize the training process. Special handling, such as resetting the discount ratio, is applied when task termination occurs during trajectory sampling.

To compute the gradient of the policy loss 
âˆ‚
â„’
Î¸
âˆ‚
Î¸
, the simulator is integrated as a differentiable layer within the PyTorch [31] computation graph, allowing for regular backpropagation. The differentiable simulator is crucial here as it enables full utilization of the underlying dynamics connecting states and actions, optimizing the policy to achieve better short-horizon rewards as well as trajectoryâ€™s overall performance. After updating the policy 
Ï€
Î¸
, the trajectories collected in the current learning episode are used to train the value function 
V
Ï•
. The value function network is trained using the following mean squared error (MSE) loss:

â„’
Ï•
=
ğ„
ğ¬
âˆˆ
{
Ï„
i
}
â¢
[
â€–
V
Ï•
â¢
(
ğ¬
)
âˆ’
V
~
â¢
(
ğ¬
)
â€–
2
]
,
(3)
where 
V
~
â¢
(
ğ¬
)
 is the estimated value of state 
s
, and is computed from the sampled short-horizon trajectories through a td-
Î»
 formulation [32].

II-B3D Gaussian Splatting
3D Gaussian Splatting (3DGS) is a technique for representing and rendering 3D scenes by leveraging a continuous and compact set of anisotropic 3D Gaussian primitives. Introduced by Kerbl et al. [13], this method models scenes with a collection of Gaussians, each parameterized by a position 
Î¼
i
âˆˆ
â„
3
, an anisotropic covariance matrix 
ğšº
i
âˆˆ
â„
3
Ã—
3
, a color 
ğœ
i
âˆˆ
â„
3
, and a opacity 
Î±
i
âˆˆ
[
0
,
1
]
. These parameters enable 3DGS to represent geometry and radiance efficiently while achieving real-time rendering.

In 3DGS, rendering involves projecting the Gaussians onto a 2D image plane and aggregating their contributions to approximate pixel colors. For a pixel 
ğ©
, the accumulated radiance is computed as:

ğ‚
â¢
(
ğ©
)
=
âˆ‘
i
=
1
N
w
i
â¢
(
ğ©
)
â‹…
ğœ
i
,
(4)
where 
w
i
â¢
(
ğ©
)
 is the weight of the 
i
-th Gaussianâ€™s contribution to 
ğ©
. This weight is determined by the Gaussian distribution:

w
i
â¢
(
ğ©
)
=
ğ’©
â¢
(
ğ©
;
Î¼
i
,
ğšº
i
)
â‹…
T
i
,
(5)
where 
ğ’©
â¢
(
ğ©
;
Î¼
i
,
ğšº
i
)
 represents the 2D projected Gaussian, and 
T
i
 is the transmittance term that handles occlusions. The transmittance is recursively defined as:

T
i
=
âˆ
j
=
1
i
âˆ’
1
(
1
âˆ’
Î±
j
)
,
(6)
where 
Î±
j
 is the opacity of the 
j
-th Gaussian.

Overall, 3DGS achieves real-time rendering with high visual fidelity, largely thanks to its point-based rasterization pipeline and the compactness of the Gaussian representation.

IIIMethod
We present a hybrid vision-differentiable simulation framework that combines 3DGS with a differentiable simulator to train robot visuomotor policies efficiently. At its core, we introduce GRaD-Nav, a DDRL algorithm tailored for end-to-end visual navigation, improving sample efficiency over prior methods. By training a unified policy across diverse 3DGS environments, our approach enhances task-level generalization, paving the way for more adaptable and autonomous mobile robots.

III-ASimulator Setting
III-A1Differentiable Quadrotor Dynamics Simulation
We implemented a parallelized, differentiable quadrotor dynamics simulator in PyTorch that computes gradients through full state transitions. Our system takes body rates 
ğ
t
d
âˆˆ
â„
3
 and normalized thrust 
c
t
âˆˆ
[
0
,
1
]
 as control inputs, and outputs the next state 
ğ’”
t
+
1
=
(
ğ’‘
,
ğ’’
,
ğ’—
,
ğ
,
ğ’‚
)
âˆˆ
â„
16
 containing position, orientation (quaternion), linear velocity, angular velocity, and acceleration. The dynamics model is fully differentiable, with 
âˆ‚
ğ’”
t
+
1
/
âˆ‚
ğ’”
t
 and 
âˆ‚
ğ’”
t
+
1
/
âˆ‚
(
ğ
t
d
,
c
t
)
 computed automatically via PyTorchâ€™s computation graph.

The core dynamics are governed by rigid-body mechanics and a PD attitude controller. For a quadrotor with mass 
m
 and inertia matrix 
ğ‘°
, the angular acceleration 
ğ
Ë™
 is computed as:

ğ‰
=
ğ‘²
p
â¢
(
ğ
d
âˆ’
ğ
)
âˆ’
ğ‘²
d
â¢
ğ
Ë™
,
(7)
ğ
Ë™
=
ğ‘°
âˆ’
1
â¢
[
ğ‰
âˆ’
ğ
Ã—
(
ğ‘°
â¢
ğ
)
]
.
(8)
Orientation is represented by a unit quaternion 
ğ’’
âˆˆ
ğ•Š
3
âŠ‚
â„
4
, which is updated through quaternion integration:

ğ’’
t
+
1
=
norm
â¢
(
ğ’’
t
+
Î”
â¢
t
2
â¢
ğ’’
t
âŠ—
[
0
â¢
ğ
]
âŠ¤
)
,
(9)
where 
âŠ—
 denotes quaternion multiplication. Linear acceleration in world frame is derived from rotation matrix 
ğ‘¹
â¢
(
ğ’’
)
 and thrust 
T
=
c
â‹…
T
max
:

ğ’‚
=
1
m
â¢
ğ‘¹
â¢
(
ğ’’
)
â¢
[
0 0
â¢
T
]
âŠ¤
+
ğ’ˆ
.
(10)
The state 
ğ’”
t
=
[
ğ’‘
t
,
ğ’—
t
,
ğ’’
t
,
ğ
t
]
 consists of position, velocity, orientation (quaternion), and angular velocity. The control input 
ğ’–
t
=
(
ğ
t
d
,
c
t
)
 includes desired body rates and normalized thrust. To achieve more accurate dynamic simulation, we also incorporated motor delay and drag force modeling in the simulator.

III-A2Hybrid simulation with 3DGS
We used a pre-trained 3DGS model to deliver the droneâ€™s first person perspective visual information and to imitate the droneâ€™s onboard RGB cameraâ€™s input during real flight in the same environment. Every step, the differentiable dynamic simulator sends the batched drone poses 
ğ“
=
(
ğ’‘
,
ğ’’
)
âˆˆ
â„
7
 to the 3DGS model and renders simulated RGB images.

Meanwhile, 3DGS also provides us with a ready-made point cloud model for the same environment, which can be used to set up reward waypoints and plan a reference trajectory for guiding the DDRL training, as well as conducting collision checks during simulation. More details will be introduced in Section III-C3.

III-A3Implementation datails
We used a standard open-source codebase, Nerfstudio [33] as our 3DGS training and inference platform. The differentiable drone dynamics model is also implemented with PyTorch, which enables efficient Jacobian computation through autograd for training the policy using our GRaD-Nav algorithm. When parallel simulating 128 drones flying in a highly unstructured and cluttered area (room size 
â‰ˆ
100
â¢
m
2
, 3DGS model size 
â‰ˆ
 1.5GB), setting the simulated time step as 0.05s (20Hz), the simulation wall-clock time is about 0.7s per step, tested on a desktop with i9-13900K CPU, RTX 4090 GPU, and 48GB RAM. The detailed account of each itemâ€™s time consumption during simulation is listed in Table I.

TABLE I:Computation time consumption ratio of different items of hybrid differentiable-3DGS simulator for each step
Item	Ratio (%)
Diff. Dynamics Sim.	33.5
3DGS Rendering	55.7
Collision Check	10.8
III-BModel Architecture
III-B1Perception network
We utilize a pretrained SqueezeNet [34] convolutional neural network (CNN) to process the RGB perception information of the drone from both 3DGS and onboard camera during training time and deployment time, respectively. SqueezeNet is a light-weighted, efficient, and robust CNN that can extract important information from droneâ€™s perception in real-time. Based on this, we finetuned a fully connected layer to downsample SqueezeNetâ€™s raw output from 512 to 24 dimensions to get the visual information embedding 
ğ
t
âˆˆ
â„
24
. This embedding is used to train policy network and context-aided estimator network along with the observation.

III-B2Policy network
The policy network 
Ï€
Î¸
â¢
(
ğš
t
+
1
|
ğ¨
t
,
ğ³
t
,
ğ
t
)
 is a 3-layer multilayer perceptron (MLP) parameterized by 
Î¸
, each layer has 512, 256, 128 neurons, where 
ğ³
t
âˆˆ
â„
16
 is the latent embedding encoded by CENet and 
ğ¨
t
âˆˆ
â„
16
 is the observation defined as following:

ğ¨
t
=
[
ğ’‰
t
ğ’’
t
ğ’—
t
ğ’‚
t
ğ’‚
t
âˆ’
1
]
T
,
(11)
where 
ğ’‰
t
, 
ğ’’
t
, 
ğ’—
t
 are drone bodyâ€™s height, quaternion, and linear velocity, respectively. 
ğ’‚
t
 and 
ğ’‚
t
âˆ’
1
 are current action and previous action. We do not need to explicitly estimate droneâ€™s x-y positions. All variables in the observation can be obtained from the droneâ€™s onboard sensors, which makes it possible to achieve 0-shot sim-to-real transfer.

III-B3Value network
The value network 
V
Ï•
â¢
(
ğ¬
t
,
ğ³
t
)
 is parameterized by 
Ï•
 and is used to estimate the state value. The value network is also a 3-layer MLP, which shares the same structure as the policy network. Except for the observation 
ğ¨
t
, the privileged observation 
ğ¬
t
âˆˆ
â„
43
 used by the value network also has access to body position 
ğ’‘
t
âˆˆ
â„
3
, depth prior 
ğ’…
t
âˆˆ
â„
24
, defined as:

ğ¬
t
=
[
ğ¨
t
ğ’‘
t
ğ’…
t
]
T
.
(12)
ğ’…
t
 is gained with 3DGSâ€™s depth image and is downsampled to the desired size by average pooling. The value function is not needed at runtime, hence the access to privileged information from the simulator is not a practical limitation.

III-B4Context estimator network
We incorporated a 
Î²
-variational autoencoder (
Î²
-VAE) [35] based CENet [30] with visual perception from the 3DGS model (training time) or real camera (deployment time). CENet is designed for encoding the droneâ€™s surrounding environment, especially the spatial relationship with obstacles, to a latent vector 
ğ³
t
 to enable runtime adaptivity to the environment. CENet is designed with an encoder-decoder structure. The encoder processes the input history observation 
ğ¨
t
H
, extracting latent environment representations 
ğ³
t
. The decoder reconstructs the subsequent observation 
ğ¨
t
+
1
. The 
Î²
-VAE objective consists of both a reconstruction loss and a latent regularization term. The full CENet loss can be expressed as:

â„’
C
â¢
E
=
MSE
(
ğ¨
^
t
+
1
,
ğ¨
t
+
1
)
+
Î²
D
K
â¢
L
(
q
ğ³
t
(
ğ³
t
|
ğ¨
t
H
)
|
|
p
(
ğ³
t
)
)
,
(13)
where 
ğ¨
^
t
+
1
 is the reconstructed observation at the next timestep, and 
q
ğ³
t
â¢
(
ğ³
t
|
ğ¨
t
H
)
 is the posterior distribution of the latent variable conditioned on the input observation. The prior distribution 
p
â¢
(
ğ³
t
)
 is assumed to follow a standard normal distribution. The backbone of our CENet is a three-layer MLP, sharing the same structure as the policy network. We used a history observation of the last 5 time steps as CENetâ€™s input, i.e. 
ğ¨
t
H
=
ğ¨
t
5
.

III-CTraining Procedure
III-C1Reward function
We designed our reward function for two main objectives, (i) smooth and stable dynamic control; (ii) efficient and safe navigation. To enable our method to better transfer to different environments and agents, we tried to avoid finetuning a complicated reward function. The details of our reward function are listed in Table II. The total stepwise reward of the policy for taking action at each state is calculated by:

r
t
â¢
(
ğ¬
t
,
ğš
t
)
=
âˆ‘
r
i
â¢
w
i
.
(14)
TABLE II:Reward function terms and their respective weights. Here, early terminations includes (i) droneâ€™s height exceeds the ceilingâ€™s height (3m), (ii) droneâ€™s linear velocity exceeds the threshold (20m/s), (iii) drone has been out of bound for more than 3m, i.e. 
ğ’™
o
.
b
.
,
ğ’š
o
.
b
.
â‰¥
3
, 
ğ’’
0
 is the droneâ€™s initial quaternion, 
ğ’‰
target
 is the target height of the drone (also serving as the initial hover height), 
ğ’š
^
yaw
 is the normalized heading direction of the drone, 
ğ’…
wp
 is the distance to the next waypoint, 
ğ’…
obst
 is the distance to the closest obstacle in the droneâ€™s field of view (FOV), 
ğ’™
o.b.
 and 
ğ’š
o.b.
 are the distances from the 3DGS map boundary, and 
ğ’—
des
 is the desired velocity direction from the pre-planned reference trajectory.
Reward	Equation (
r
i
)	Weight (
w
i
)
Safe Control Rewards
\hdashlineSurvival 	
âˆ‰
{
early terminations
}
8.0
Linear velocity	
â€–
ğ’—
â€–
2
âˆ’
0.5
Pose	
â€–
ğ’’
âˆ’
ğ’’
0
â€–
âˆ’
0.5
Height	
(
ğ’‰
âˆ’
ğ’‰
target
)
2
âˆ’
2.0
Action	
â€–
ğ’‚
t
â€–
2
âˆ’
1.0
Action rate	
â€–
ğ’‚
t
âˆ’
ğ’‚
t
âˆ’
1
â€–
2
âˆ’
1.0
Smoothness	
â€–
ğ’‚
t
âˆ’
2
â¢
ğ’‚
t
âˆ’
1
+
ğ’‚
t
âˆ’
2
â€–
2
âˆ’
1.0
Efficient Navigation Rewards
\hdashlineYaw alignment 	
ğ’—
x
â¢
y
â€–
ğ’—
x
â¢
y
â€–
â‹…
ğ’š
^
yaw
0.25
Waypoint	
exp
â¡
(
âˆ’
ğ’…
wp
)
2.0
Obstacle avoidance	
ğ’…
obst
1.0
Out-of-map	
ğ’™
o.b.
2
+
ğ’š
o.b.
2
âˆ’
2.0
Ref. traj. tracking	
â€–
ğ’—
â€–
ğ’—
â€–
âˆ’
ğ’—
des
â€–
ğ’—
ref
â€–
â€–
âˆ’
2.0
III-C2Domain randomization
Domain randomization serves as a standard approach today in robotics reinforcement learning. By changing the environmentâ€™s appearance, dynamics, and sensor noise, the agent can learn a more robust policy that generalizes to unseen environments and achieves better sim-to-real transitions. The randomization parameters are listed in Table III.

TABLE III:Parameter randomization ranges and units.
Parameter	Randomization range	Unit
Mass	
[
1.0
,
1.5
]
kg
Max. thrust	
[
22
,
30
]
N
I
x
,
I
y
 inertial 	
[
0.075
,
0.125
]
kg
â‹…
m
2
I
z
 inertial 	
[
0.15
,
0.25
]
kg
â‹…
m
2
Motor delay factor	
[
0.5
,
0.8
]
-
Body rate delay factor	
[
0.5
,
0.8
]
-
Drag force coefficient	
[
0.4
,
0.6
]
-
III-C3Guiding drone to fly desired trajectories
In our reinforcement learning framework for drone navigation, we incorporate multiple reward terms to encourage the drone to fly through obstacles while ensuring collision avoidance. A key component of our approach is defining waypoints and precomputing a reference trajectory based on a point cloud generated from a 3DGS representation of the environment.

One critical reward component is the waypoint reward 
r
wp
, which encourages the drone to approach the next waypoint on the reference trajectory. The reward is formulated as:

r
wp
=
(
e
âˆ’
â€–
ğ’‘
âˆ’
ğ’˜
next
â€–
2
)
,
(15)
where 
ğ’‘
 is the current drone position, 
ğ’˜
next
 represents the next waypoint (i.e., the closest waypoint with 
x
wp
>
x
drone
).

To further reinforce trajectory tracking, we introduce a reference trajectory tracking reward 
r
traj
, which penalizes deviations between the normalized velocity of the drone and the desired velocity along the trajectory:

r
traj
=
â€–
ğ’—
â€–
ğ’—
â€–
âˆ’
ğ’—
des
â€–
ğ’—
des
â€–
â€–
,
(16)
where 
ğ’—
â€–
ğ’—
â€–
 is the normalized velocity of the drone, 
ğ’—
des
â€–
ğ’—
des
â€–
 is the desired velocity direction drawn from the reference trajectory.

Additionally, to ensure safe navigation, we incorporate an obstacle avoidance reward 
r
obs
, which encourages the drone to maintain a safe distance from obstacles visible in its FOV. The reward formulation is as follows:

r
obs
=
ğ’…
obs
,
ğ’…
obs
<
ğ’…
th
,
(17)
where 
ğ’…
obs
 represents the minimum distance to the nearest obstacle in droneâ€™s FOV, 
ğ’…
th
 is the predefined threshold for proximity, which is set to 0.5m in our method.

These reward components contribute to the overall reward function, which balances waypoint tracking, trajectory adherence, and obstacle avoidance to optimize drone navigation in complex environments. The detailed reward factors 
Ï‰
i
 for each term are listed in II.

III-C4Curriculum training for generalizable navigation policy
Beyond training a single policy for a long horizon trajectory, our method can also train generalizable policies that can adapt to different surrounding environments and conduct safe navigation in them. We achieve end-to-end generalizable gate position detection and safe navigation with a curriculum training method. We have three different 3DGS environments for training, each one with a unique gate position. By rolling training across these different environments, we finally trained a policy that can adapt to different gate positions and achieve generalizable navigation. The reward function (Table. II) is kept the same during training. We trained the policy in each environment for 5 times, 100 epochs per time; we return the learning rate to the initial value during every environment transition, other hyperparameters are listed in Table VI.

IVExperimental Results
IV-ATraining efficiency comparison with other methods
To validate GRaD-Navâ€™s training efficiency, we compared our method with other differentiable and non-differentiable RL algorithms. Back Propagation Through Time (BPTT) [36] is a critic-free differentiable reinforcement learning algorithm that passes gradients through the whole trajectory; Proximal Policy Optimization (PPO) [37] is a common non-differentiable, online reinforcement learning algorithm. All three algorithms were used for training policies that navigate through a trajectory of around 20m, passing through several challenging unstructured obstacles, which can be seen in 3. As much as possible, we used identical hyperparameters (Table VI) for different algorithms. It is to be noted that BPTT samples the whole trajectory for policy updating, meaning the horizon length equals the episode length, which can take up a lot of memory for each environment and heavily limits its maximum allowed number of parallel simulated environments. The experiment results in Fig.2 show that (i) non-differentiable RL can struggle to train a satisfactory policy for this end-to-end visual navigation task; (ii) compared to vanilla BPTT, our method achieves over 300% sample efficiency and uses only 20% of training time to deliver a better policy.

Refer to caption
Figure 2:Sample efficiency and wall-clock time comparison benchmark of different algorithms on droneâ€™s vision-based end-to-end navigation policy training.
IV-BAblation study of our methods
To validate that each module of our method is not redundant but necessary for safe navigation, and to determine each moduleâ€™s contribution to the entire training framework, we conduct a comprehensive ablation test to train the policy networks with certain modules ablated on two different surrounding environments. We train each policy with the same reward function as in Table II and the same hyperparameters setting as Table VI for 600 epochs. Our ablation test metrics include: (i) training reward, (ii) test reward, and (iii) test success rate. Training reward is the highest reward policy achieved during training time. After training, we roll out the best policy and conduct simulations for 10 drones in parallel. The simulated droneâ€™s initial positions are randomized in a cube with side length of 1m, centered at (0,0,1.3)m; their initial poses are also randomized, 
Ï•
,
Î¸
,
Ïˆ
âˆˆ
[
âˆ’
0.25
,
0.25
]
.
 Other dynamic properties of the simulated drones are also randomized as in Table III. The assessments of a success trajectory include: (i) not suffering from early termination as discussed in Table II; (ii) getting close enough (
â€–
ğ’‘
âˆ’
ğ’˜
next
â€–
2
m
â¢
i
â¢
n
â‰¤
0.3
â¢
m
) to each waypoint in sequence; (iii) keeping a safe distance (
â‰¥
 0.2m) to any obstacles on the trajectory.

Refer to caption
Figure 3:Example success trajectories in hybrid simulation environments achieved by the proposed method. The left one is â€œmiddle gateâ€ and the right one is â€œright gateâ€, aligning with the ablation study Table IV.
TABLE IV:Simulation results of ablation test, the reward function is defined in Table II, each method is trained with GRaD-Nav algorithm and hyperparameters list in Table VI. Example trajectories are shown in Fig. 3.
Trajectories	Middle gate	Right gate
Methods	Training reward	Evaluation reward	Success rate	Training reward	Evaluation reward	Success rate
w/o visual obs.	3720.1	3761.4 
Â±
 60.1	0/10	4162.4	4148.3 
Â±
 76.3	0/10
w/o RGB; w/ depth	3828.6	3805.7 
Â±
 45.9	0/10	4168.2	4161.7 
Â±
 27.5	0/10
w/o velocity	3579.6	3430.1 
Â±
 116.8	0/10	4030.1	4012.0 
Â±
 113.4	0/10
w/o CENet	4014.0	4124.3 
Â±
 140.9	4/10	4641.7	4583.9 
Â±
 238.1	5/10
Proposed	4622.1	4893.6 
Â±
 24.8	8/10	4928.5	4718.3 
Â±
 12.6	7/10
The experiment results show that our proposed method achieves the highest training and evaluation rewards as well as success rate on both trajectories among all methods. As visual perception is our navigation policyâ€™s major sensor input, it is not surprising that the policy without visual observation cannot conduct successful navigation. Using depth image rather than RGB image also demonstrated a low success rate in vision-ablated experiments, which emphasized the necessity of accessing RGB information for effective navigation on our task. As introduced in Section III-C3, droneâ€™s body linear velocity plays an important role in our policyâ€™s navigation-related reward items, velocity-ablated policies would suffer from serious compounding error, and thus cannot fly an ideal trajectory. Without CENet, our method can still train a policy network that achieves high rewards compared to other ablation cases. CENet ablated policies can complete a trajectory successfully when the initial condition is close to the â€neutralâ€ initial condition (e.g. position = (0,0,1.3)m). However, policies without CENet can be vulnerable to any environmental perturbations, which seriously harm the robustness of the policy networks. All of the failure cases without CENet on two trajectories â€crashâ€ due to unsuccessful obstacle avoidance.

IV-CSim-to-real transfer of generalizable policy
After training our policy as discussed in Section III-C4, we rolled out the final policy and tested it in three different environments separately. Without any prior environmental-related information like the mapâ€™s name, gate position, or reference trajectory, the policy can only access the first person perspective RGB information rendered from 3DGS. During experiments, the rolled-out policy could â€recallâ€ what it learned in different surrounding environments and guide the drone to fly through the gate that is placed at different locations. Fig. 4 demonstrates the generalizable policyâ€™s variant trajectories in different environments.

Refer to caption
Figure 4:Generalizable policy to fly through gates at different positions and with different distractor objects.
Thanks to the high fidelity first person perspective RGB image rendered with 3DGS as showed in Fig. 6, we can conduct convenient zero-shot sim-to-real transfer. Our drone is mounted with a Pixracer low-level flight controller, a Nvidia Jetson Orin Nano onboard computer, and an Intel Realsense D435 camera. The onboard policy inference frequency is 30 Hz. Fig. 7 is a demonstration of the successful trajectories navigating through gates with variant y-positions. Fig. 6 further validates CENetâ€™s role in environment understanding. We recorded the latent vector 
ğ³
t
 at three flight stagesâ€”approaching, passing through, and after the gateâ€”and applied PCA. The latent vectors during passage show a more compact distribution than the other stages. By comparing three methodsâ€™ real robot test performance in Table V, we can conclude that (i) the sim-to-real gap of our method is reasonably low; (ii) CENet is essential in delivering robust sim-to-real transfer; (iii) only using depth data in real robot deployment would lead to a severe loss of environment detail thus seriously harming the success rate.

Refer to caption
Figure 5:Comparison on droneâ€™s first person perspective image rendered with 3DGS in simulator (left) and captured with Intel Realsense D435 camera in real robot deployment (right).
Refer to caption
Figure 6:The droneâ€™s first-person views at three different stages during the real-robot experiment, along with the PCA visualization of the latent vector 
ğ³
t
 from the context encoder CENet at each stage.
Refer to caption
Figure 7:Robot hardware experiments of drone flying through middle gate.
TABLE V:Experimental results of generalizable policies trained using different methods. In simulation experiments, the droneâ€™s randomized initialization settings and the evaluation metrics for successful trajectories are consistent with those in Section IV-B. In real-world experiments, the drone was initialized under the same conditions for each test, and success was determined by whether it flew through the gate without collision.
Methods	Success rate (sim. | real)
Left gate	Middle gate	Right gate
w/o RGB; w/ depth	4/10 | 1/10	7/10 | 1/10	5/10 | 0/10
w/o CENet	9/10 | 0/10	10/10 | 2/10	7/10 | 1/10
Proposed	10/10 | 7/10	10/10 | 7/10	9/10 | 6/10
VConclusions
In this paper, we present a novel framework that integrates 3DGS with DDRL to train a vision-based drone navigation policy. By leveraging high-fidelity 3D scene representations and differentiable simulation, our approach enhances sample efficiency and sim-to-real transfer. Experimental results demonstrate that our method outperforms existing approaches in training efficiency. Our method also demonstrates robustness and generalization ability.

Limitations: Our method requires significant reward shaping, e.g. with hand-engineered trajectory waypoints. This also leads to policies that are adept at a single task (e.g., flying through a gate), but not able to execute multiple tasks. Our policy also requires a reliable onboard estimate of velocity, which can be obtained from lower-level VIO modules, which can give inconsistent performance in practice. Future work can focus on various aspects, including (i) training a multi-task policy with language conditioning; (ii) further improving our policyâ€™s generalization ability by upgrading backbone policy network and training in a greater varieties of surrounding environments; (iii) extending this work to more contact-rich scenarios like holistic mobile manipulation and aerial manipulation.